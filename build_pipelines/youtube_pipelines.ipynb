{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc6b928",
   "metadata": {},
   "source": [
    "This notebook will take the Youtube dataset made of csv files and json files and write the data to local database first.\n",
    "\n",
    "Then it will write the data to AWS database using the AWS wrangler connection, and pyspark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f7c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The multi imports that are required for this project\n",
    "\n",
    "#The pumping equipment of the pipeline\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Work with JSON files\n",
    "import json\n",
    "\n",
    "#Work with AWS\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b168426",
   "metadata": {},
   "source": [
    "There are two sets of files we are going to work with. \n",
    "\n",
    "- CSV files that are in separate folders based on the regions. Files themselves don't have the region names include inside them\n",
    "\n",
    "- Json files that are in single folder with region names present inside the files.\n",
    "\n",
    "In real world such files or sources needs to be brought together in pipelines, joined correctly and then loaded into the final database / sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6485c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvsource = \"/home/solverbot/Desktop/ytDE/csvfiles\"\n",
    "jsonsource= \"/home/solverbot/Desktop/ytDE/jsonfiles\"\n",
    "\n",
    "# require the below libraries functions to write out the parquets\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "parquetPath = \"/home/solverbot/Desktop/ytDE/parquetSink\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd6a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 13:09:03 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.64.83 instead (on interface wlo1)\n",
      "23/01/27 13:09:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/01/27 13:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# We will initiate the spark session with the parameters necessary to \n",
    "# make connection with the database.\n",
    "\n",
    "spark = SparkSession.builder.appName(\"YT_Pipeline\"). \\\n",
    "            config('spark.jars',\"/usr/share/java/postgresql-42.2.26.jar\"). \\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first see where the raw files are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e422ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh \n",
    "cd /home/solverbot/Desktop/ytDE/csvfiles\n",
    "ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d24c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To reduce the typing\n",
    "sparkC = spark.sparkContext #rarely used\n",
    "sparksql = spark.sql\n",
    "filereader = spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d407f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#implementing the recursive filelook for the csv files\n",
    "\n",
    "youtubeCSV_table = filereader.csv(path=csvsource,\n",
    "                                 recursiveFileLookup=True,\n",
    "                                 header=True,\n",
    "                                 inferSchema=True) \\\n",
    "                    .withColumn(\"region\",input_file_name().substr(46,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "\n",
    "youtubeCSV_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a954d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the region column\n",
    "\n",
    "youtubeCSV_cleaned = youtubeCSV_table.selectExpr(\"*\", \"split_part(region, '/',1) as location\") \\\n",
    "                .drop(\"region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubeCSV_cleaned.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubeCSV_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99ec3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubeCSV_sample = youtubeCSV_cleaned.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e961dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubeCSV_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubeCSV_sample_pandas = youtubeCSV_sample.to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubeCSV_sample_pandas.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8d825",
   "metadata": {},
   "source": [
    "### Using AWS Wrangler to \n",
    "\n",
    "1) Write the files to dataframe to S3-bucket\n",
    "\n",
    "2) Write the table details to Glue catalog and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae72e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = configparser.ConfigParser()\n",
    "reader.read_file(open('calter.config'))\n",
    "\n",
    "reg = reader[\"AWS\"][\"REGION\"]\n",
    "key = reader[\"AWS\"][\"KEY\"]\n",
    "sec = reader[\"AWS\"][\"SECRET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9429bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a session into the aws account\n",
    "\n",
    "boto_session = boto3.Session(region_name=reg,aws_access_key_id=key,\n",
    "                            aws_secret_access_key=sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1b9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubeCSV_sample_pandas = youtubeCSV_sample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9144db2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://pipe-line-source/youtube.parquet/c4d06ded20f047859192c356e49050db.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(dataset=True,df=youtubeCSV_sample_pandas,\n",
    "                path='s3://pipe-line-source/youtube.parquet',\n",
    "                boto3_session=boto_session,\n",
    "                 mode='append',\n",
    "                description=\"That took time to understand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06436857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://boto-bucket-16/aa002f8520fe4288976b10b567e0be5e.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(dataset=True,df=youtubeCSV_sample_pandas,\n",
    "                path='s3://boto-bucket-16/',\n",
    "                boto3_session=boto_session,\n",
    "                database='youtube_data',table='sample_csv',\n",
    "                 mode='append',\n",
    "                description=\"That took time to understand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below command will write out the data to postgress database\n",
    "youtubeCSV_cleaned.write.format('jdbc') \\\n",
    "                .option(\"url\", \"jdbc:postgresql://pipeline-tank.coc5gkht2i7a.us-east-1.rds.amazonaws.com:5432/pipeline_exercise\") \\\n",
    "                .option('dbtable','yt_csv') \\\n",
    "                .option('user','postgres') \\\n",
    "                .option('password', 'wrangler') \\\n",
    "                .option('driver','org.postgresql.Driver') \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb8ce0",
   "metadata": {},
   "source": [
    "Ensure the security group is configured to allow public network traffic into the database. Use the manual or the python automation script.\n",
    "\n",
    "I have reviewed the connection using the postgres client on my local machine. You can check that using the DBeaver / SQLWorkbench IDE too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets concentrate on the json files\n",
    "\n",
    "youtubejsonRaw = filereader.json(path=jsonsource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7000f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spark throws error that record is corrupt !!!\n",
    "youtubejsonRaw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at what is the reason for corruption using shell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "head -n 15 CA_category_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "tail -n 15 CA_category_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668311bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquetMaker(file_name: str):\n",
    "    \"\"\"The function recieves the json filename and \n",
    "    converts to parquet and writes it to parquet\n",
    "    folder.\n",
    "    \n",
    "    Ensure parquet folder in present in the path\"\"\"\n",
    "    filepath = jsonsource + f'/{file_name}'\n",
    "    dest_file = parquetPath+\"/\"+ file_name.split('.')[0] + '.parquet'\n",
    "    print(dest_file)\n",
    "    try:\n",
    "        # Creating DF from content\n",
    "        df_raw = pd.read_json(filepath)\n",
    "        df_step_1 = pd.json_normalize(df_raw['items'])\n",
    "        df_step_1.columns = ['kind','etag','id','channelId','title','assignable']\n",
    "        df_step_1.to_parquet(path=dest_file)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cf404",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetMaker(\"CA_category_id.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c62929",
   "metadata": {},
   "outputs": [],
   "source": [
    "newParquet = filereader.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newParquet.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c90c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the os and glob module to work with multiple file\n",
    "import os\n",
    "import glob\n",
    "\n",
    "jsonGlob = glob.glob(root_dir=jsonsource,pathname=\"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loop moves the json files through the function\n",
    "#writes out the parquets\n",
    "for jsonfile in jsonGlob:\n",
    "    parquetMaker(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e74dd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jsonParquetDf = filereader.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2978f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonParquetDf.createOrReplaceTempView(\"jsondataframe_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b03e37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           channelId|\n",
      "+--------------------+\n",
      "|UCBR8-60-B28hp2Bm...|\n",
      "|UCBR8-60-B28hp2Bm...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#To make the reference easier for future, clean col names\n",
    "sparksql(\"\"\"SELECT channelId FROM jsondataframe_view LIMIT 2\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c6281",
   "metadata": {},
   "source": [
    "Lets first write these two dataframes to the RDS instance in AWS\n",
    "\n",
    "There are two ways to do it. \n",
    "\n",
    "- Using the SparkSession itself \n",
    "\n",
    "- Using the AWS Wrangler\n",
    "\n",
    "In addition, we can write these tables s3 buckets, and in parallel\n",
    "register them in Glue Catalog for Athena to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb38bf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>etag</th>\n",
       "      <th>id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>title</th>\n",
       "      <th>assignable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtube#videoCategory</td>\n",
       "      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/Xy1mB4_yLrHy_BmKm...</td>\n",
       "      <td>1</td>\n",
       "      <td>UCBR8-60-B28hp2BmDPdntcQ</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube#videoCategory</td>\n",
       "      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/UZ1oLIIz2dxIhO45Z...</td>\n",
       "      <td>2</td>\n",
       "      <td>UCBR8-60-B28hp2BmDPdntcQ</td>\n",
       "      <td>Autos &amp; Vehicles</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    kind                                               etag  \\\n",
       "0  youtube#videoCategory  \"m2yskBQFythfE4irbTIeOgYYfBU/Xy1mB4_yLrHy_BmKm...   \n",
       "1  youtube#videoCategory  \"m2yskBQFythfE4irbTIeOgYYfBU/UZ1oLIIz2dxIhO45Z...   \n",
       "\n",
       "  id                 channelId             title  assignable  \n",
       "0  1  UCBR8-60-B28hp2BmDPdntcQ  Film & Animation        True  \n",
       "1  2  UCBR8-60-B28hp2BmDPdntcQ  Autos & Vehicles        True  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of just moving the parquet file seperately to s3 and then \n",
    "# write the table catalog, it can be done in single go.\n",
    "\n",
    "jsonparquet_pandas = jsonParquetDf.toPandas()\n",
    "jsonparquet_pandas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a75cd158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://boto-bucket-16/987bc9ce6dc347fb93f15a65416cb028.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(database='youtube_data',table='json_category',\n",
    "                dataset=True,path='s3://boto-bucket-16/',\n",
    "                boto3_session=boto_session,mode='append',\n",
    "                description=\"json data table\",df=jsonparquet_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8c943",
   "metadata": {},
   "source": [
    "Based on the research, writing the data out to s3/ hdfs as text would be the most effective way of working with the AWS.\n",
    "\n",
    "- The sink S3 can be recorded inside the Glue catalog, and then Athena can be used to query it for data and use it for processing\n",
    "\n",
    "- Processing can be use it with some additional transformation, or to use it with quicksight for visualisation\n",
    "\n",
    "\n",
    "Connecting the RDS / Other databases to the Athena/ Glue will require considerable effort if the set up done manually with out using the cloud formation. \n",
    "\n",
    "I am thinking of understanding how the cloud formation works, and created. After that further exploration on connecting the databases will be taken up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6ae2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f595aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfd843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
