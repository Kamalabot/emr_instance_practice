{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc6b928",
   "metadata": {},
   "source": [
    "This notebook will take the Youtube dataset made of csv files and json files and write the data to local database first.\n",
    "\n",
    "Then it will write the data to AWS database using the AWS wrangler connection, and pyspark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74f7c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The multi imports that are required for this project\n",
    "\n",
    "#The pumping equipment of the pipeline\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Work with JSON files\n",
    "import json\n",
    "\n",
    "#Work with AWS\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b168426",
   "metadata": {},
   "source": [
    "There are two sets of files we are going to work with. \n",
    "\n",
    "- CSV files that are in separate folders based on the regions. Files themselves don't have the region names include inside them\n",
    "\n",
    "- Json files that are in single folder with region names present inside the files.\n",
    "\n",
    "In real world such files or sources needs to be brought together in pipelines, joined correctly and then loaded into the final database / sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6485c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvsource = \"/home/solverbot/Desktop/ytDE/csvfiles\"\n",
    "jsonsource= \"/home/solverbot/Desktop/ytDE/jsonfiles\"\n",
    "\n",
    "# require the below libraries functions to write out the parquets\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "parquetPath = \"/home/solverbot/Desktop/ytDE/parquetSink\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd6a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 11:03:17 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.64.83 instead (on interface wlo1)\n",
      "23/01/27 11:03:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/01/27 11:03:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# We will initiate the spark session with the parameters necessary to \n",
    "# make connection with the database.\n",
    "\n",
    "spark = SparkSession.builder.appName(\"YT_Pipeline\"). \\\n",
    "            config('spark.jars',\"/usr/share/java/postgresql-42.2.26.jar\"). \\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first see where the raw files are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e422ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh \n",
    "cd /home/solverbot/Desktop/ytDE/csvfiles\n",
    "ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d24c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To reduce the typing\n",
    "sparkC = spark.sparkContext #rarely used\n",
    "sparksql = spark.sql\n",
    "filereader = spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d407f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#implementing the recursive filelook for the csv files\n",
    "\n",
    "youtubeCSV_table = filereader.csv(path=csvsource,\n",
    "                                 recursiveFileLookup=True,\n",
    "                                 header=True,\n",
    "                                 inferSchema=True) \\\n",
    "                    .withColumn(\"region\",input_file_name().substr(46,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f6b138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+---------------+\n",
      "|   video_id|trending_date|               title|channel_title|category_id|        publish_time|                tags| views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|         region|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+---------------+\n",
      "|gDuslQ9avLc|     17.14.11|Захар и Полина уч...|    Т—Ж БОГАЧ|         22|2017-11-13T09:09:...|\"захар и полина|\"...| 62408|  334|     190|           50|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|Знакомьтесь, это ...|RU/RUvideos.csv|\n",
      "|AOCJIFEA_jE|     17.14.11|Биржа Мемов #29. ...| Druzhko Show|         22|2017-11-13T17:32:...|\"биржа мемов|\"\"ле...|330043|43841|    2244|         2977|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|В 29 выпуске Друж...|RU/RUvideos.csv|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "\n",
    "youtubeCSV_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a954d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the region column\n",
    "\n",
    "youtubeCSV_cleaned = youtubeCSV_table.selectExpr(\"*\", \"split_part(region, '/',1) as location\") \\\n",
    "                .drop(\"region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ee7fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+--------+\n",
      "|   video_id|trending_date|               title|channel_title|category_id|        publish_time|                tags| views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|location|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+--------+\n",
      "|gDuslQ9avLc|     17.14.11|Захар и Полина уч...|    Т—Ж БОГАЧ|         22|2017-11-13T09:09:...|\"захар и полина|\"...| 62408|  334|     190|           50|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|Знакомьтесь, это ...|      RU|\n",
      "|AOCJIFEA_jE|     17.14.11|Биржа Мемов #29. ...| Druzhko Show|         22|2017-11-13T17:32:...|\"биржа мемов|\"\"ле...|330043|43841|    2244|         2977|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|В 29 выпуске Друж...|      RU|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "youtubeCSV_cleaned.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ca283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "416869"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtubeCSV_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e7e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubeCSV_sample = youtubeCSV_cleaned.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509ba731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtubeCSV_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f1afd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "youtubeCSV_sample_pandas = youtubeCSV_sample.to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35532d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gDuslQ9avLc</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Захар и Полина учатся экономить</td>\n",
       "      <td>Т—Ж БОГАЧ</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T09:09:31.000Z</td>\n",
       "      <td>\"захар и полина|\"\"учимся экономить\"\"|\"\"копить ...</td>\n",
       "      <td>62408</td>\n",
       "      <td>334</td>\n",
       "      <td>190</td>\n",
       "      <td>50</td>\n",
       "      <td>https://i.ytimg.com/vi/gDuslQ9avLc/default.jpg</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Знакомьтесь, это Захар и Полина. Вместе с ними...</td>\n",
       "      <td>RU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOCJIFEA_jE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Биржа Мемов #29. Большой выпуск</td>\n",
       "      <td>Druzhko Show</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:32:11.000Z</td>\n",
       "      <td>\"биржа мемов|\"\"лев шагинян\"\"|\"\"мемы\"\"|\"\"пикчи\"...</td>\n",
       "      <td>330043</td>\n",
       "      <td>43841</td>\n",
       "      <td>2244</td>\n",
       "      <td>2977</td>\n",
       "      <td>https://i.ytimg.com/vi/AOCJIFEA_jE/default.jpg</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>В 29 выпуске Дружко Шоу Сергей сказал, что Бир...</td>\n",
       "      <td>RU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date                             title channel_title category_id              publish_time                                                                                                                                                                                                                                                                              tags   views  likes dislikes comment_count                                  thumbnail_link comments_disabled ratings_disabled video_error_or_removed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         description location\n",
       "0  gDuslQ9avLc      17.14.11   Захар и Полина учатся экономить     Т—Ж БОГАЧ          22  2017-11-13T09:09:31.000Z  \"захар и полина|\"\"учимся экономить\"\"|\"\"копить деньги\"\"|\"\"экономь\"\"|\"\"мелкие траты\"\"|\"\"как экономить на еде\"\"|\"\"советы экономить\"\"|\"\"личный бюджет\"\"|\"\"при маленькой зарплате\"\"|\"\"свадьба\"\"|\"\"подготовка к свадьбе\"\"|\"\"гости на свадьбе\"\"|\"\"красивая свадьба\"\"|\"\"жизнь свадьба\"\"\"   62408    334      190            50  https://i.ytimg.com/vi/gDuslQ9avLc/default.jpg             FALSE            FALSE                  FALSE                                                                                                                                       Знакомьтесь, это Захар и Полина. Вместе с ними мы сняли серию роликов о семейном бюджете, экономии и тратах. В первом ролике ребята готовятся к свадьбе и хотят отпраздновать ее пышно. Но без накоплений, с одной зарплатой на двоих и кучей мелких трат свадьба может не случиться. Смотрите, как спасти семейный бюджет и научиться копить. В следующем ролике расскажем, как заработать больше, если денег все равно не хватает.\\n\\nТаблица для ежедневного бюджета: https://journal.tinkoff.ru/spreadsheet/\\n\\nПодписывайтесь на наш канал и узнавайте, как с умом пользоваться деньгами:  https://www.youtube.com/tinkoffjournal\\n\\nГотовы рассказать свою историю или стать автором канала — пишите на rich@tinkoff.ru       RU\n",
       "1  AOCJIFEA_jE      17.14.11  Биржа Мемов #29. Большой выпуск  Druzhko Show          22  2017-11-13T17:32:11.000Z                                                                       \"биржа мемов|\"\"лев шагинян\"\"|\"\"мемы\"\"|\"\"пикчи\"\"|\"\"дружко\"\"|\"\"дружко шоу\"\"|\"\"анонимус\"\"|\"\"борщ\"\"|\"\"ftp\"\"|\"\"мхк\"\"|\"\"на случай важных переговоров\"\"|\"\"аффлек\"\"|\"\"бэн аффлек\"\"|\"\"batman\"\"|\"\"бэтмен\"\"|\"\"трамп\"\"\"  330043  43841     2244          2977  https://i.ytimg.com/vi/AOCJIFEA_jE/default.jpg             FALSE            FALSE                  FALSE  В 29 выпуске Дружко Шоу Сергей сказал, что Биржа Мемов отдельно выйдет на его канале в понедельник…и не соврал!\\n\\nВстречайте, расширенная и, впервые, самостоятельная Биржа Мемов, в которой Лев вспомнил лучшие пикчи последней недели, обсудил самые мемные новости, встретился с очередным гостем и разобрал один из пабликов ВК на орпригодность в рамках своей новой рубрики! В общем, обязательно к просмотру.\\n\\nВедущий биржи мемов: \\nVK: https://vk.cc/6Dolqn\\nInstagram: https://goo.gl/AoHGPN\\n\\nБиржа Мемов:\\nhttps://vk.com/stockmem\\n\\nГруппа Вконтакте: https://vk.cc/6BBS07\\nГруппа Facebook: https://goo.gl/ZS5mzT\\nInstagram Шоу: https://goo.gl/F5s5PK\\n\\nМоя личная страница ВКонтакте: https://vk.cc/6Djxew\\nМой инстаграм: https://goo.gl/LukuVV\\n\\nИван #ЖаданИсполняетМечты:\\nVK: https://vk.cc/70ncIe\\nInstagram: https://goo.gl/DVjC87       RU"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtubeCSV_sample_pandas.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5db9a",
   "metadata": {},
   "source": [
    "#### Write out the dataframe as parquet to s3_bucket\n",
    "\n",
    "Create the bucket using boto3 resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae72e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    }
   ],
   "source": [
    "reader = configparser.ConfigParser()\n",
    "reader.read_file(open('calter.config'))\n",
    "\n",
    "reg = reader[\"AWS\"][\"REGION\"]\n",
    "key = reader[\"AWS\"][\"KEY\"]\n",
    "sec = reader[\"AWS\"][\"SECRET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34dba0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a session into the aws account\n",
    "\n",
    "boto_session = boto3.Session(region_name=reg,aws_access_key_id=key,\n",
    "                            aws_secret_access_key=sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(dataset=True,df=youtubeCSV_sample_pandas,\n",
    "                partition_cols=\"location\",\n",
    "                path='s3://pipe-line-source/youtube_sample',\n",
    "                database='youtube_data',\n",
    "                table='youtube_csv_sample',\n",
    "                boto3_session=boto_session,mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed47035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Below command will write out the data to postgress database\n",
    "youtubeCSV_cleaned.write.format('jdbc') \\\n",
    "                .option(\"url\", \"jdbc:postgresql://pipeline-tank.coc5gkht2i7a.us-east-1.rds.amazonaws.com:5432/pipeline_exercise\") \\\n",
    "                .option('dbtable','yt_csv') \\\n",
    "                .option('user','postgres') \\\n",
    "                .option('password', 'wrangler') \\\n",
    "                .option('driver','org.postgresql.Driver') \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb8ce0",
   "metadata": {},
   "source": [
    "Ensure the security group is configured to allow public network traffic into the database. Use the manual or the python automation script.\n",
    "\n",
    "I have reviewed the connection using the postgres client on my local machine. You can check that using the DBeaver / SQLWorkbench IDE too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c23db5",
   "metadata": {},
   "source": [
    "### Using AWS Wrangler to \n",
    "\n",
    "1) Write the files to dataframe to S3-bucket\n",
    "\n",
    "2) Write the table details to Glue catalog and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cb510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a265a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97b694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b58bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets concentrate on the json files\n",
    "\n",
    "youtubejsonRaw = filereader.json(path=jsonsource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7000f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spark throws error that record is corrupt !!!\n",
    "youtubejsonRaw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at what is the reason for corruption using shell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "head -n 15 CA_category_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "tail -n 15 CA_category_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "668311bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquetMaker(file_name: str):\n",
    "    \"\"\"The function recieves the json filename and \n",
    "    converts to parquet and writes it to parquet\n",
    "    folder.\n",
    "    \n",
    "    Ensure parquet folder in present in the path\"\"\"\n",
    "    filepath = jsonsource + f'/{file_name}'\n",
    "    dest_file = parquetPath+\"/\"+ file_name.split('.')[0] + '.parquet'\n",
    "    print(dest_file)\n",
    "    try:\n",
    "        # Creating DF from content\n",
    "        df_raw = pd.read_json(filepath)\n",
    "        df_step_1 = pd.json_normalize(df_raw['items'])\n",
    "        df_step_1.columns = ['kind','etag','id','channelId','title','assignable']\n",
    "        df_step_1.to_parquet(path=dest_file)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d23cf404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/solverbot/Desktop/ytDE/parquetSink/CA_category_id.parquet\n"
     ]
    }
   ],
   "source": [
    "parquetMaker(\"CA_category_id.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47c62929",
   "metadata": {},
   "outputs": [],
   "source": [
    "newParquet = filereader.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac2e193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+---------+-----+----------+\n",
      "|                kind|                etag| id|channelId|title|assignable|\n",
      "+--------------------+--------------------+---+---------+-----+----------+\n",
      "|youtube#videoCate...|\"m2yskBQFythfE4ir...|  1|     null| null|      null|\n",
      "|youtube#videoCate...|\"m2yskBQFythfE4ir...|  2|     null| null|      null|\n",
      "+--------------------+--------------------+---+---------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newParquet.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c90c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the os and glob module to work with multiple file\n",
    "import os\n",
    "import glob\n",
    "\n",
    "jsonGlob = glob.glob(root_dir=jsonsource,pathname=\"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36a5f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/solverbot/Desktop/ytDE/parquetSink/GB_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/JP_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/RU_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/FR_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/CA_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/US_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/DE_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/KR_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/MX_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/IN_category_id.parquet\n"
     ]
    }
   ],
   "source": [
    "#This loop moves the json files through the function\n",
    "#writes out the parquets\n",
    "for jsonfile in jsonGlob:\n",
    "    parquetMaker(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e74dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonParquetDf = filereader.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d2978f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonParquetDf.createOrReplaceTempView(\"jsondataframe_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b03e37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           channelId|\n",
      "+--------------------+\n",
      "|UCBR8-60-B28hp2Bm...|\n",
      "|UCBR8-60-B28hp2Bm...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To make the reference easier for future, clean col names\n",
    "sparksql(\"\"\"SELECT channelId FROM jsondataframe_view LIMIT 2\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c6281",
   "metadata": {},
   "source": [
    "Lets first write these two dataframes to the RDS instance in AWS\n",
    "\n",
    "There are two ways to do it. \n",
    "\n",
    "- Using the SparkSession itself \n",
    "\n",
    "- Using the AWS Wrangler\n",
    "\n",
    "In addition, we can write these tables s3 buckets, and in parallel\n",
    "register them in Glue Catalog for Athena to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38bf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
