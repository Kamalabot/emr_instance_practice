{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc6b928",
   "metadata": {},
   "source": [
    "This notebook will take the Youtube dataset made of csv files and json files and write the data to local database first.\n",
    "\n",
    "Then it will write the data to AWS database using the AWS wrangler connection, and pyspark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f7c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The multi imports that are required for this project\n",
    "\n",
    "#The pumping equipment of the pipeline\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Work with JSON files\n",
    "import json\n",
    "\n",
    "#Work with AWS\n",
    "import awswrangler\n",
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b168426",
   "metadata": {},
   "source": [
    "There are two sets of files we are going to work with. \n",
    "\n",
    "- CSV files that are in separate folders based on the regions. Files themselves don't have the region names include inside them\n",
    "\n",
    "- Json files that are in single folder with region names present inside the files.\n",
    "\n",
    "In real world such files or sources needs to be brought together in pipelines, joined correctly and then loaded into the final database / sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6485c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvsource = \"/home/solverbot/Desktop/ytDE/csvfiles\"\n",
    "jsonsource= \"/home/solverbot/Desktop/ytDE/jsonfiles\"\n",
    "\n",
    "# require the below libraries functions to write out the parquets\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "parquetPath = \"/home/solverbot/Desktop/ytDE/parquetSink\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd6a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 08:01:39 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.64.83 instead (on interface wlo1)\n",
      "23/01/27 08:01:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/01/27 08:01:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# We will initiate the spark session with the parameters necessary to \n",
    "# make connection with the database.\n",
    "\n",
    "spark = SparkSession.builder.appName(\"YT_Pipeline\"). \\\n",
    "            config('spark.jars',\"/usr/share/java/postgresql-42.2.26.jar\"). \\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first see where the raw files are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e422ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh \n",
    "cd /home/solverbot/Desktop/ytDE/csvfiles\n",
    "ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d24c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To reduce the typing\n",
    "sparkC = spark.sparkContext #rarely used\n",
    "sparksql = spark.sql\n",
    "filereader = spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d407f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#implementing the recursive filelook for the csv files\n",
    "\n",
    "youtubeCSV_table = filereader.csv(path=csvsource,\n",
    "                                 recursiveFileLookup=True,\n",
    "                                 header=True,\n",
    "                                 inferSchema=True) \\\n",
    "                    .withColumn(\"region\",input_file_name().substr(46,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f6b138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+---------------+\n",
      "|   video_id|trending_date|               title|channel_title|category_id|        publish_time|                tags| views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|         region|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+---------------+\n",
      "|gDuslQ9avLc|     17.14.11|Захар и Полина уч...|    Т—Ж БОГАЧ|         22|2017-11-13T09:09:...|\"захар и полина|\"...| 62408|  334|     190|           50|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|Знакомьтесь, это ...|RU/RUvideos.csv|\n",
      "|AOCJIFEA_jE|     17.14.11|Биржа Мемов #29. ...| Druzhko Show|         22|2017-11-13T17:32:...|\"биржа мемов|\"\"ле...|330043|43841|    2244|         2977|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|В 29 выпуске Друж...|RU/RUvideos.csv|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "\n",
    "youtubeCSV_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a954d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the region column\n",
    "\n",
    "youtubeCSV_cleaned = youtubeCSV_table.selectExpr(\"*\", \"split_part(region, '/',1) as location\") \\\n",
    "                .drop(\"region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ee7fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+--------+\n",
      "|   video_id|trending_date|               title|channel_title|category_id|        publish_time|                tags| views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|location|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+--------+\n",
      "|gDuslQ9avLc|     17.14.11|Захар и Полина уч...|    Т—Ж БОГАЧ|         22|2017-11-13T09:09:...|\"захар и полина|\"...| 62408|  334|     190|           50|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|Знакомьтесь, это ...|      RU|\n",
      "|AOCJIFEA_jE|     17.14.11|Биржа Мемов #29. ...| Druzhko Show|         22|2017-11-13T17:32:...|\"биржа мемов|\"\"ле...|330043|43841|    2244|         2977|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|В 29 выпуске Друж...|      RU|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "youtubeCSV_cleaned.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef42585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================>                       (3 + 2) / 5]\r"
     ]
    }
   ],
   "source": [
    "youtubeCSV_cleaned.write.format('jdbc') \\\n",
    "                .option(\"url\", \"jdbc:postgresql://pipeline-tank.coc5gkht2i7a.us-east-1.rds.amazonaws.com:5432/pipeline_exercise\") \\\n",
    "                .option('dbtable','yt_csv') \\\n",
    "                .option('user','postgres') \\\n",
    "                .option('password', 'wrangler') \\\n",
    "                .option('driver','org.postgresql.Driver') \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = configparser.ConfigParser()\n",
    "reader.read_file(open('calter.config'))\n",
    "\n",
    "reg = reader[\"AWS\"][\"REGION\"]\n",
    "key = reader[\"AWS\"][\"KEY\"]\n",
    "sec = reader[\"AWS\"][\"SECRET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d6c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.session(region=reg,aws_access_key_id=key,\n",
    "                            aws_secret_access_key=sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets concentrate on the json files\n",
    "\n",
    "youtubejsonRaw = filereader.json(path=jsonsource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7000f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spark throws error that record is corrupt !!!\n",
    "youtubejsonRaw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at what is the reason for corruption using shell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "head -n 15 CA_category_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd /home/solverbot/Desktop/ytDE/jsonfiles\n",
    "tail -n 15 CA_category_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "668311bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquetMaker(file_name: str):\n",
    "    \"\"\"The function recieves the json filename and \n",
    "    converts to parquet and writes it to parquet\n",
    "    folder.\n",
    "    \n",
    "    Ensure parquet folder in present in the path\"\"\"\n",
    "    filepath = jsonsource + f'/{file_name}'\n",
    "    dest_file = parquetPath+\"/\"+ file_name.split('.')[0] + '.parquet'\n",
    "    print(dest_file)\n",
    "    try:\n",
    "        # Creating DF from content\n",
    "        df_raw = pd.read_json(filepath)\n",
    "        df_step_1 = pd.json_normalize(df_raw['items'])\n",
    "        df_step_1.columns = ['kind','etag','id','channelId','title','assignable']\n",
    "        df_step_1.to_parquet(path=dest_file)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d23cf404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/solverbot/Desktop/ytDE/parquetSink/CA_category_id.parquet\n"
     ]
    }
   ],
   "source": [
    "parquetMaker(\"CA_category_id.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47c62929",
   "metadata": {},
   "outputs": [],
   "source": [
    "newParquet = filereader.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac2e193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+---------+-----+----------+\n",
      "|                kind|                etag| id|channelId|title|assignable|\n",
      "+--------------------+--------------------+---+---------+-----+----------+\n",
      "|youtube#videoCate...|\"m2yskBQFythfE4ir...|  1|     null| null|      null|\n",
      "|youtube#videoCate...|\"m2yskBQFythfE4ir...|  2|     null| null|      null|\n",
      "+--------------------+--------------------+---+---------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newParquet.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c90c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the os and glob module to work with multiple file\n",
    "import os\n",
    "import glob\n",
    "\n",
    "jsonGlob = glob.glob(root_dir=jsonsource,pathname=\"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36a5f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/solverbot/Desktop/ytDE/parquetSink/GB_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/JP_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/RU_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/FR_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/CA_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/US_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/DE_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/KR_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/MX_category_id.parquet\n",
      "/home/solverbot/Desktop/ytDE/parquetSink/IN_category_id.parquet\n"
     ]
    }
   ],
   "source": [
    "#This loop moves the json files through the function\n",
    "#writes out the parquets\n",
    "for jsonfile in jsonGlob:\n",
    "    parquetMaker(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e74dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonParquetDf = filereader.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d2978f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonParquetDf.createOrReplaceTempView(\"jsondataframe_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b03e37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           channelId|\n",
      "+--------------------+\n",
      "|UCBR8-60-B28hp2Bm...|\n",
      "|UCBR8-60-B28hp2Bm...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To make the reference easier for future, clean col names\n",
    "sparksql(\"\"\"SELECT channelId FROM jsondataframe_view LIMIT 2\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c6281",
   "metadata": {},
   "source": [
    "Lets first write these two dataframes to the RDS instance in AWS\n",
    "\n",
    "There are two ways to do it. \n",
    "\n",
    "- Using the SparkSession itself \n",
    "\n",
    "- Using the AWS Wrangler\n",
    "\n",
    "In addition, we can write these tables s3 buckets, and in parallel\n",
    "register them in Glue Catalog for Athena to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38bf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
