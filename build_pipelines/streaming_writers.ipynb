{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ba2318",
   "metadata": {},
   "source": [
    "We are going to work on interesting topic of streaming data. The idea of this notebook is as follows\n",
    "\n",
    "- Take a streaming stock market input that is provided by the netcat server. (Python program is written for the same)\n",
    "\n",
    "- Data is caught by the spark stream reader, and schema is detected\n",
    "\n",
    "- The data is then written to the local database first\n",
    "\n",
    "- Writing the data to file using the for each and for each batch\n",
    "\n",
    "- Working on joining multiple streaming information together. \n",
    "(Need think about the dataset that is sent in parallel from another netcat server)\n",
    "\n",
    "\n",
    "* Compelete the python share_pusher.py and weather_pusher.py scripts\n",
    "\n",
    "The server will have \n",
    "\n",
    "tail_logs.sh | nc -lvnp 9999\n",
    "\n",
    "The reciever will have \n",
    "\n",
    "nc -vn 127.0.0.1 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c1dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32adb591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 21:18:52 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "23/01/27 21:18:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/01/27 21:18:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "##Lets simply read the data from socket\n",
    "\n",
    "socket = SparkSession.builder.appName(\"socket-reader\"). \\\n",
    "        config(\"spark.jars\",\"/usr/share/java/postgresql-42.2.26.jar\"). \\\n",
    "        master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f6ca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 21:20:37 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "socketReader = socket.readStream.format('socket'). \\\n",
    "                option(\"host\",\"localhost\"). \\\n",
    "                option(\"port\",\"9999\"). \\\n",
    "                load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9963dd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "socketReader.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285fb4a",
   "metadata": {},
   "source": [
    "The below query is built step by step, by checking the write stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14012b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_count = socketReader.select(split('value',' ')[0].alias('addr'),\n",
    "                               split(split('value',' ')[3],\":\")[0].alias('date'),\n",
    "                               split('value',' ')[6].alias('dept')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97733bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c3452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_count.writeStream.format(\"console\").outputMode(\"append\"). \\\n",
    "    trigger(processingTime='5 seconds').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_count = socketReader.filter(split('value',' ')[6].contains('department')) \\\n",
    "                        .select(split('value',' ')[0].alias('addr'),\n",
    "                               split(split('value',' ')[3],\":\")[0].alias('date'),\n",
    "                               split('value',' ')[6].alias('dept')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03626be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_count.writeStream.format(\"console\").outputMode('append'). \\\n",
    "            trigger(processingTime='5 seconds').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec05b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_count.writeStream.format(\"console\").outputMode('append'). \\\n",
    "            trigger(processingTime='5 seconds'). \\\n",
    "            option(\"truncate\",\"false\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db255c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_filter = socketReader.filter(split(split('value',' ')[6],'/')[1] == 'department') \\\n",
    "                        .select(split('value',' ')[0].alias('addr'),\n",
    "                               split(split('value',' ')[3],\":\")[0].alias('date'),\n",
    "                               split('value',' ')[6].alias('dept')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbae6bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 21:21:36 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a7042b99-283f-4a53-a041-e13f2899b87c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/27 21:21:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f67ab3ab6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+----+----+\n",
      "|addr|date|dept|\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------+------------+------------------------------------+\n",
      "|addr         |date        |dept                                |\n",
      "+-------------+------------+------------------------------------+\n",
      "|131.3.183.100|[27/Jan/2023|/department/team%20sports/categories|\n",
      "|47.199.148.5 |[27/Jan/2023|/department/footwear/categories     |\n",
      "|81.179.176.92|[27/Jan/2023|/department/footwear/products       |\n",
      "+-------------+------------+------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------------+------------+-------------------------------+\n",
      "|addr         |date        |dept                           |\n",
      "+-------------+------------+-------------------------------+\n",
      "|207.239.55.95|[27/Jan/2023|/department/fan%20shop/products|\n",
      "+-------------+------------+-------------------------------+\n",
      "\n",
      "23/01/27 21:21:47 WARN TextSocketMicroBatchStream: Stream closed by localhost:9999\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------+------------+-------------------------------+\n",
      "|addr          |date        |dept                           |\n",
      "+--------------+------------+-------------------------------+\n",
      "|173.189.89.198|[27/Jan/2023|/department/outdoors/categories|\n",
      "+--------------+------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exact_filter.writeStream.format(\"console\").outputMode('append'). \\\n",
    "            trigger(processingTime='5 seconds'). \\\n",
    "            option(\"truncate\",\"false\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e36a5a",
   "metadata": {},
   "source": [
    "What is happening with foreach is very interesting!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15aeacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgres(row):\n",
    "    print(type(row))\n",
    "    print(row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "679c9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_batch(row,batch):\n",
    "    print(row)\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db5b87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It connected to database, and then threw error\n",
    "# Each row is acting like a new dataframe\n",
    "def write_to_db(row,batch):\n",
    "    row.write.format('jdbc') \\\n",
    "                .option(\"url\", \"jdbc:postgresql://localhost:5432/maintenance\") \\\n",
    "                .option('dbtable','streaming_rice') \\\n",
    "                .option('user','postgres') \\\n",
    "                .option('password', '1234') \\\n",
    "                .option('driver','org.postgresql.Driver') \\\n",
    "                .save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b40590e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 21:46:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ef017c78-9aa7-4b51-918b-8dda532b9dbd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/27 21:46:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f676b662b00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '189.32.98.223', 'date': '[27/Jan/2023', 'dept': '/department/outdoors/products'}\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '23.200.115.206', 'date': '[27/Jan/2023', 'dept': '/department/fan%20shop/categories'}<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '82.229.233.192', 'date': '[27/Jan/2023', 'dept': '/department/fitness/products'}\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '128.33.114.221', 'date': '[27/Jan/2023', 'dept': '/department/golf/categories'}\n",
      "\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '219.149.174.10', 'date': '[27/Jan/2023', 'dept': '/department/team%20sports/categories'}\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '87.7.174.197', 'date': '[27/Jan/2023', 'dept': '/department/team%20sports/categories'}\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '134.202.158.81', 'date': '[27/Jan/2023', 'dept': '/department/fitness/products'}\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '211.230.17.206', 'date': '[27/Jan/2023', 'dept': '/department/team%20sports/products'}\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '178.124.113.171', 'date': '[27/Jan/2023', 'dept': '/department/fan%20shop/categories'}\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "{'addr': '35.199.189.245', 'date': '[27/Jan/2023', 'dept': '/department/golf/categories'}\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 21:46:58 WARN TextSocketMicroBatchStream: Stream closed by localhost:9999\n"
     ]
    }
   ],
   "source": [
    "exact_filter.writeStream.foreach(write_to_postgres).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5547cab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 21:59:51 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ca0cbdc1-9d91-43d2-8018-b7366f21fb6b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/27 21:59:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f676a6ac700>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/27 22:00:00 ERROR MicroBatchExecution: Query [id = 75c7336b-5423-4cab-966a-e296d0e7d4fd, runId = f2f69064-46b0-4ebf-b98f-c71c044294fc] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/solverbot/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/solverbot/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/home/solverbot/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_200706/1714206912.py\", line 8, in write_to_db\n",
      "    .save()\n",
      "  File \"/home/solverbot/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 966, in save\n",
      "    self._jwrite.save()\n",
      "  File \"/home/solverbot/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/solverbot/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 196, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: Table or view 'streaming_rice' already exists. SaveMode: ErrorIfExists.\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy34.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    }
   ],
   "source": [
    "exact_filter.writeStream.outputMode('append'). \\\n",
    "        foreachBatch(write_to_db). \\\n",
    "        trigger(processingTime='10 seconds').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_filter.write.getItem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_group = socketReader.filter(split(split('value',' ')[6],'/')[1] == 'department') \\\n",
    "                        .select(split('value',' ')[0].alias('addr'),\n",
    "                               split(split('value',' ')[3],\":\")[0].alias('date'),\n",
    "                               split(split('value',' ')[6],'/')[2].alias('dept')) \\\n",
    "                        .groupby('dept').agg(count(lit(1)).alias('dept_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac6c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_group.writeStream.format(\"console\").outputMode(\"complete\"). \\\n",
    "        trigger(processingTime='5 seconds'). \\\n",
    "        option(\"truncate\",\"false\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d5e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_group.writeStream.format(\"console\").outputMode(\"update\"). \\\n",
    "        trigger(processingTime='15 seconds'). \\\n",
    "        option(\"truncate\",\"false\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd808a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketReader.createOrReplaceTempView(\"socketReader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "socket.sql(\"SHOW VIEWS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketReader.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "socket.sql(\"\"\"SELECT * FROM socketReader\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a637c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_get_group = socket.sql(\"\"\"\n",
    "    SELECT SPLIT(SPLIT(value, ' ')[6], '/')[2] AS dept,\n",
    "    COUNT(value) as dept_count,\n",
    "    FROM socketReader\n",
    "    WHERE SPLIT(SPLIT(value, ' ')[6], '/')[1] = 'department'\n",
    "    GROUP BY SPLIT(SPLIT(value, ' ')[6], '/')[2]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketReader. \\\n",
    "    writeStream. \\\n",
    "    format('csv'). \\\n",
    "    option(\"checkpointLocation\", f'/run/media/solverbot/repoA/checkpoint'). \\\n",
    "    option('path', f'/run/media/solverbot/repoA/data'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec53b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
